{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655ef666",
   "metadata": {
    "id": "655ef666"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5ea5b5",
   "metadata": {
    "id": "db5ea5b5"
   },
   "outputs": [],
   "source": [
    "# Load SVHN dataset\n",
    "data = scipy.io.loadmat(\"train_32x32.mat\")\n",
    "X_train = data['X']\n",
    "Y_train = data['y']\n",
    "\n",
    "data = scipy.io.loadmat(\"test_32x32.mat\")\n",
    "X_test = data['X']\n",
    "Y_test = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00feae1e",
   "metadata": {
    "id": "00feae1e",
    "outputId": "94b7433b-2e05-48e4-cdd5-0802bd7aee79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3072, 73257)\n",
      "Y_train shape: (10, 73257)\n",
      "X_test shape: (3072, 26032)\n",
      "Y_test shape: (10, 26032)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the images\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1]*X_train.shape[2], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1]*X_test.shape[2], -1)\n",
    "\n",
    "# Normalize the images\n",
    "\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255.\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "\n",
    "Y_train = np.eye(10)[Y_train.flatten()-1].T\n",
    "Y_test = np.eye(10)[Y_test.flatten()-1].T\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BKq-B6EB8Yrt",
   "metadata": {
    "id": "BKq-B6EB8Yrt"
   },
   "source": [
    "## Here we do the following splitting a dataset into training and testing sets, and then reshaping the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a009a683",
   "metadata": {
    "id": "a009a683",
    "outputId": "79580718-1a68-45f8-f806-238cc5f41d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3072, 1000)\n",
      "Y_train shape: (10, 1000)\n",
      "X_test shape: (3072, 400)\n",
      "Y_test shape: (3072, 400)\n"
     ]
    }
   ],
   "source": [
    "# assigning the arrays to the variables \n",
    "a = X_train.T\n",
    "b = X_test.T\n",
    "c = Y_train.T\n",
    "d = Y_test.T\n",
    "\n",
    "#lists to store the data\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "#selecting a subset of the data to use as the training and testing sets.\n",
    "# where the first 1000 rows of a to X_train, the first 350 rows of b to X_test, the first 1000 rows of c to Y_train, and the first 350 rows of d to Y_test\n",
    "for i in range(1000):\n",
    "    X_train.append(a[i])\n",
    "    \n",
    "for i in range(400):\n",
    "    X_test.append(b[i])\n",
    "    \n",
    "for i in range(1000):\n",
    "    Y_train.append(c[i])\n",
    "    \n",
    "for i in range(400):\n",
    "    Y_test.append(d[i])\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(X_test)\n",
    "# transposing the arrays back to their original shape.\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T\n",
    "\n",
    "X_train = X_train\n",
    "Y_train = Y_train\n",
    "X_test = X_test\n",
    "Y_test = Y_test\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E6YLyut--fSS",
   "metadata": {
    "id": "E6YLyut--fSS"
   },
   "source": [
    "## defining the activation functions which will be used for classification if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7328158",
   "metadata": {
    "id": "e7328158"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX/np.sum(expX, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0238d2",
   "metadata": {
    "id": "bf0238d2"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SnAD9d32ILbh",
   "metadata": {
    "id": "SnAD9d32ILbh"
   },
   "source": [
    "## Xavier weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "KewGud7ZH3jt",
   "metadata": {
    "id": "KewGud7ZH3jt"
   },
   "outputs": [],
   "source": [
    "# Define Xavier weight initialization function\n",
    "def xavier_init(n_in, n_out):\n",
    "    # return np.random.normal(0, np.sqrt(2/(n_in + n_out)), (n_in, n_out))\n",
    "    return np.random.randn(n_in, n_out) * np.sqrt(1/n_in)\n",
    "def xavier_init(n_in, n_out):\n",
    "    return np.random.randn(n_out, int(n_in / np.sqrt(n_in)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SXtHXFl6_WwE",
   "metadata": {
    "id": "SXtHXFl6_WwE"
   },
   "source": [
    "## initializing the parameters of a neural network with three hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8b0cdf",
   "metadata": {
    "id": "2a8b0cdf"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, neurons_hidden, n_y):\n",
    "    np.random.seed(2)\n",
    "    w1 = np.random.randn(neurons_hidden, n_x) * np.sqrt(1 / n_x)\n",
    "    b1 = np.zeros((neurons_hidden, 1))\n",
    "    w2 = np.random.randn(neurons_hidden, neurons_hidden) * np.sqrt(1 /neurons_hidden)\n",
    "    b2 = np.zeros((neurons_hidden, 1))\n",
    "    w3 = np.random.randn(n_y, neurons_hidden) * np.sqrt(1 /neurons_hidden)\n",
    "    b3 = np.zeros((n_y, 1))\n",
    "    parameters = {\"w1\": w1, \n",
    "                  \"b1\": b1, \n",
    "                  \"w2\": w2, \n",
    "                  \"b2\": b2,\n",
    "                  \"w3\": w3, \n",
    "                  \"b3\": b3}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BiYDPaetA4Rp",
   "metadata": {
    "id": "BiYDPaetA4Rp"
   },
   "source": [
    "## forward_propagation that performs the forward propagation step of a neural network with three hidden layers. Computing the activations of the hidden layers and output layer.forward_cache containing the pre-activations z1, z2, and z3, as well as the activations a1, a2, and a3, and returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e713646",
   "metadata": {
    "id": "1e713646"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(x, parameters):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    w3 = parameters['w3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    \n",
    "    z1 = np.dot(w1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    \n",
    "    forward_cache = {\n",
    "        \"z1\" : z1,\n",
    "        \"a1\" : a1,\n",
    "        \"z2\" : z2,\n",
    "        \"a2\" : a2,\n",
    "        \"z3\" : z3,\n",
    "        \"a3\" : a3\n",
    "    }\n",
    "    \n",
    "    return forward_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vtdvmaj6BeHH",
   "metadata": {
    "id": "vtdvmaj6BeHH"
   },
   "source": [
    "defines a cost function to measure the error between the predicted output A2 and the actual output Y. It calculates the cross-entropy loss by computing the negative log-likelihood of the predicted output A2 given the actual output Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a449df19",
   "metadata": {
    "id": "a449df19"
   },
   "outputs": [],
   "source": [
    "def cost_function(A2, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs_1 = np.multiply(np.log(A2), Y)\n",
    "    logprobs_2 = np.multiply(np.log(1 - A2),(1 - Y))\n",
    "    logprobs = logprobs_1 + logprobs_2\n",
    "    cost = - (1/m) * np.sum(logprobs)\n",
    "    \n",
    "    cost = float(np.squeeze(cost))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hs-ebzvyCce0",
   "metadata": {
    "id": "Hs-ebzvyCce0"
   },
   "source": [
    "defines a function backward_propagation that computes the gradients of the cost function with respect to the parameters of the neural network using the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db9723f",
   "metadata": {
    "id": "2db9723f"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(x, y, parameters, forward_cache):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    w3 = parameters['w3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    a1 = forward_cache['a1']\n",
    "    a2 = forward_cache['a2']\n",
    "    a3 = forward_cache['a3']\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    dz3 = (a3 - y)\n",
    "    dw3 = (1/m)*np.dot(dz3, a2.T)\n",
    "    db3 = (1/m)*np.sum(dz3, axis = 1, keepdims = True)\n",
    "    \n",
    "    dz2 = (1/m)*np.dot(w3.T, dz3)*sigmoid_derivative(a2)\n",
    "    dw2 = (1/m)*np.dot(dz2, a1.T)\n",
    "    db2 = (1/m)*np.sum(dz2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dz1 = (1/m)*np.dot(w2.T, dz2)*sigmoid_derivative(a1)\n",
    "    dw1 = (1/m)*np.dot(dz1, x.T)\n",
    "    db1 = (1/m)*np.sum(dz1, axis = 1, keepdims = True)\n",
    "    \n",
    "    gradients = {\n",
    "        \"dw1\" : dw1,\n",
    "        \"db1\" : db1,\n",
    "        \"dw2\" : dw2,\n",
    "        \"db2\" : db2,\n",
    "        \"dw3\" : dw3,\n",
    "        \"db3\" : db3\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "By4urnmaC3ja",
   "metadata": {
    "id": "By4urnmaC3ja"
   },
   "source": [
    "defines a function update_parameters_sgd that updates the values of the parameters of a neural network using stochastic gradient descent (SGD). The function first extracts the current values of the parameters and gradients from the corresponding dictionaries. It then updates the parameters using the formula parameter = parameter - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c94c415",
   "metadata": {
    "id": "8c94c415"
   },
   "outputs": [],
   "source": [
    "def update_parameters_sgd(parameters, gradients, learning_rate):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    w3 = parameters['w3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    dw1 = gradients['dw1']\n",
    "    db1 = gradients['db1']\n",
    "    dw2 = gradients['dw2']\n",
    "    db2 = gradients['db2']\n",
    "    dw3 = gradients['dw3']\n",
    "    db3 = gradients['db3']\n",
    "    \n",
    "    w1 = w1 - learning_rate*dw1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    w2 = w2 - learning_rate*dw2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    w3 = w3 - learning_rate*dw3\n",
    "    b3 = b3 - learning_rate*db3\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1,\n",
    "        \"b1\" : b1,\n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2,\n",
    "        \"w3\" : w3,\n",
    "        \"b3\" : b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moclYzLND3pk",
   "metadata": {
    "id": "moclYzLND3pk"
   },
   "source": [
    "updates the parameters of a neural network using the RMSprop optimization algorithm. The function takes four input arguments: Parametrs, gradients, learning_rate, epsilon and The RMSprop algorithm updates the weights and biases using a moving average of the squared gradients.the algorithm updates the weights and biases using the formula:\n",
    "\n",
    "W = W - learning_rate * dW / (sqrt(s_dW) + epsilon)\n",
    "\n",
    "\n",
    "b = b - learning_rate * db / (sqrt(s_db) + epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84be993",
   "metadata": {
    "id": "c84be993"
   },
   "outputs": [],
   "source": [
    "def update_parameters_rmsprop(parameters, gradients, learning_rate, epsilon=1e-8):\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary\n",
    "    w1 = parameters[\"w1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    w2 = parameters[\"w2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    w3 = parameters[\"w3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # Retrieve the gradients from the dictionary\n",
    "    dw1 = gradients[\"dw1\"]\n",
    "    db1 = gradients[\"db1\"]\n",
    "    dw2 = gradients[\"dw2\"]\n",
    "    db2 = gradients[\"db2\"]\n",
    "    dw3 = gradients[\"dw3\"]\n",
    "    db3 = gradients[\"db3\"]\n",
    "    \n",
    "    s_dW1 = np.zeros_like(dw1)\n",
    "    s_db1 = np.zeros_like(db1)\n",
    "    s_dW2 = np.zeros_like(dw2)\n",
    "    s_db2 = np.zeros_like(db2)\n",
    "    s_dW3 = np.zeros_like(dw3)\n",
    "    s_db3 = np.zeros_like(db3)\n",
    "    \n",
    "    beta = 0.9\n",
    "    \n",
    "    s_dW1 = beta * s_dW1 + (1 - beta) * dw1 ** 2\n",
    "    s_db1 = beta * s_db1 + (1 - beta) * db1 ** 2\n",
    "    s_dW2 = beta * s_dW2 + (1 - beta) * dw2 ** 2\n",
    "    s_db2 = beta * s_db2 + (1 - beta) * db2 ** 2\n",
    "    s_dW3 = beta * s_dW3 + (1 - beta) * dw3 ** 2\n",
    "    s_db3 = beta * s_db3 + (1 - beta) * db3 ** 2\n",
    "    \n",
    "    w1 -= learning_rate * dw1 / (np.sqrt(s_dW1) + epsilon)\n",
    "    b1 -= learning_rate * db1 / (np.sqrt(s_db1) + epsilon)\n",
    "    w2 -= learning_rate * dw2 / (np.sqrt(s_dW2) + epsilon)\n",
    "    b2 -= learning_rate * db2 / (np.sqrt(s_db2) + epsilon)\n",
    "    w3 -= learning_rate * dw3 / (np.sqrt(s_dW3) + epsilon)\n",
    "    b3 -= learning_rate * db3 / (np.sqrt(s_db3) + epsilon)\n",
    "    \n",
    "    parameters = {\"w1\": w1, \n",
    "                  \"b1\": b1, \n",
    "                  \"w2\": w2, \n",
    "                  \"b2\": b2, \n",
    "                  \"w3\": w3, \n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qaBjlaKEysm",
   "metadata": {
    "id": "5qaBjlaKEysm"
   },
   "source": [
    "defines a function called model that trains a neural network with a specified number of hidden neurons, learning rate, activation function, and number of iterations. The function takes in input features x and target labels y and returns the trained parameters and the cost of the network over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3782708b",
   "metadata": {
    "id": "3782708b"
   },
   "outputs": [],
   "source": [
    "def model(x, y, neurons_hidden, learning_rate, iterations, activation):\n",
    "    \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, neurons_hidden, n_y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        forward_cache = forward_propagation(x, parameters)\n",
    "        \n",
    "        cost = cost_function(forward_cache['a3'], y)\n",
    "        \n",
    "        gradients = backward_propagation(x, y, parameters, forward_cache)\n",
    "        \n",
    "        if (activation == \"SGD\"):\n",
    "            parameters = update_parameters_sgd(parameters, gradients, learning_rate)\n",
    "        if (activation == \"RMS_prop\"):\n",
    "            parameters = update_parameters_rmsprop(parameters, gradients, learning_rate, epsilon=1e-8)\n",
    "        \n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"Cost after\", i, \"iterations is :\", cost)\n",
    "        \n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qExBziaFFIlb",
   "metadata": {
    "id": "qExBziaFFIlb"
   },
   "source": [
    "training a neural network using the RMSProp optimizer, with a hidden layer of 1200 neurons, a learning rate of 0.001, and 250 iterations of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc5c9e8b",
   "metadata": {
    "id": "dc5c9e8b",
    "outputId": "76b6a87d-f63b-4f21-812d-4fd664794429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations is : 3.3769013263821077\n",
      "Cost after 25 iterations is : 3.4381587045488358\n",
      "Cost after 50 iterations is : 3.6818816512653374\n",
      "Cost after 75 iterations is : 3.388904649692596\n",
      "Cost after 100 iterations is : 3.5908095208082256\n",
      "Cost after 125 iterations is : 3.36527803877349\n",
      "Cost after 150 iterations is : 3.540197363680468\n",
      "Cost after 175 iterations is : 3.3501992787701385\n",
      "Cost after 200 iterations is : 3.502662252602062\n",
      "Cost after 225 iterations is : 3.3326132340288193\n"
     ]
    }
   ],
   "source": [
    "iterations = 250\n",
    "neurons_hidden = 1200\n",
    "learning_rate = 0.001\n",
    "\n",
    "Parameters, Cost_list = model(X_train, Y_train, neurons_hidden = neurons_hidden, learning_rate = learning_rate, iterations = iterations, activation=\"RMS_prop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RTUqqRrrFjMa",
   "metadata": {
    "id": "RTUqqRrrFjMa"
   },
   "source": [
    "function accuracy calculates the accuracy of the predictions made by a neural network model on a given input data and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f0aa98",
   "metadata": {
    "id": "95f0aa98"
   },
   "outputs": [],
   "source": [
    "def accuracy(inp, labels, parameters):\n",
    "    forward_cache = forward_propagation(inp, parameters)\n",
    "    a_out = forward_cache['a3']   # containes propabilities with shape(10, 1)\n",
    "    \n",
    "    a_out = np.argmax(a_out, 0)  # 0 represents row wise \n",
    "    \n",
    "    labels = np.argmax(labels, 0)\n",
    "    accuracy = np.mean(a_out == labels)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPM3CiCMFzOr",
   "metadata": {
    "id": "kPM3CiCMFzOr"
   },
   "source": [
    "calling the accuracy() function to compute the accuracy of the trained neural network model on both the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63f58f62",
   "metadata": {
    "id": "63f58f62",
    "outputId": "211c548f-5dc4-401c-b058-f63394c4acc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Dataset 13.600000000000001 %\n",
      "Accuracy of Test Dataset 0.5 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Train Dataset\", accuracy(X_train, Y_train, Parameters), \"%\")\n",
    "print(\"Accuracy of Test Dataset\", accuracy(X_test, Y_test, Parameters), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X3-1QaQ0GcwI",
   "metadata": {
    "id": "X3-1QaQ0GcwI"
   },
   "source": [
    "Resources: few websites where they built neural network from scratch and few github resources also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1796cfd",
   "metadata": {
    "id": "a1796cfd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
